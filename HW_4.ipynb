{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elka97/GB_PyTorch/blob/main/HW_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_lI6CRy3tNm"
      },
      "source": [
        "## Домашнее задание\n",
        "\n",
        "1. Обучите CNN (самописная) на CIFAR-100.\n",
        "2. Обучите CNN на CIFAR-100 через дообучение ImageNet Resnet-50.\n",
        "3. *Обучите CNN на CIFAR-100 через дообучение ImageNet Resnet-50 с аугментацией данных."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# imports"
      ],
      "metadata": {
        "id": "9XulMPQXfX-q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v5dpvBRrekDP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from PIL import Image\n",
        "from torchvision import transforms, datasets\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torchsummary import summary\n",
        "from torch.nn.modules import activation\n",
        "\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# load data"
      ],
      "metadata": {
        "id": "Oh_6azZd3iZD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-ufCAnXXEHX",
        "outputId": "7eb89ad6-75a6-40bc-9129-6786faf77556"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "train_data = datasets.CIFAR100(root='data/', train=True, download=True,)\n",
        "valid_data = datasets.CIFAR100(root='data/', train=False, download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKwa4BxkXX7y",
        "outputId": "c8cfcdc2-ad70-4eea-d9dd-9a7325d46271"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset CIFAR100\n",
              "    Number of datapoints: 50000\n",
              "    Root location: data/\n",
              "    Split: Train"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoAi8vZ73xDa",
        "outputId": "8e2ea3bd-aa63-4219-f340-8a7a61ef85f4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset CIFAR100\n",
              "    Number of datapoints: 10000\n",
              "    Root location: data/\n",
              "    Split: Test"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# constants"
      ],
      "metadata": {
        "id": "GnF4DUhi5MiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "num_epochs=10\n",
        "print_every = 300\n",
        "\n",
        "img_size=32 \n",
        "batch_size=128\n",
        "num_classes=len(train_data.classes)\n",
        "\n",
        "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "device, img_size, batch_size, num_classes, num_epochs, print_every"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgOIQFPg5Q1w",
        "outputId": "552b692f-9106-4bfb-fc6f-101a161108b4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('cuda', 32, 128, 100, 10, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "lr_cnn = [1e-3, 1e-4]\n",
        "lr_cnn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bX00MwvwOC1t",
        "outputId": "8ae4015d-54e2-410b-f7e4-e503f2394314"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.001, 0.0001]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for reproducibility\n",
        "torch.manual_seed(13) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmGITXEMLpdG",
        "outputId": "2926cdae-28a0-4c85-cd91-82623c7f3683"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f38cb1c47f0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data pre-processing/loaders"
      ],
      "metadata": {
        "id": "5isAMRsPsQQc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "S04vtRqcekDU"
      },
      "outputs": [],
      "source": [
        "class MyOwnCifar(torch.utils.data.Dataset):\n",
        "   \n",
        "    def __init__(self, init_dataset, transform=None):\n",
        "        self._base_dataset = init_dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._base_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self._base_dataset[idx][0]\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        return img, self._base_dataset[idx][1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataloaders(train_df, test_df, train_transformations, test_transformations):\n",
        "  _train_ds = MyOwnCifar(train_df, train_transformations)\n",
        "  _valid_ds = MyOwnCifar(test_df, test_transformations)\n",
        "\n",
        "  _train_ldr = torch.utils.data.DataLoader(_train_ds,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=True,\n",
        "                            num_workers=2)\n",
        "  _valid_ldr = torch.utils.data.DataLoader(_valid_ds,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=False,\n",
        "                            num_workers=1)\n",
        "  \n",
        "  return _train_ldr, _valid_ldr"
      ],
      "metadata": {
        "id": "qXVWC-13wPBe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# means/std for normalization\n",
        "stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))"
      ],
      "metadata": {
        "id": "--bPMsC-dOvg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data normalization only\n",
        "train_transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize(*stats,inplace=True)])\n",
        "valid_transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize(*stats)])"
      ],
      "metadata": {
        "id": "7w0oabkGdKEL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, valid_loader = get_dataloaders(train_data, valid_data, train_transforms, valid_transforms)"
      ],
      "metadata": {
        "id": "Lkq4IHgSfNCp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "lS0a0ZymsPPs",
        "outputId": "9322b90c-708d-4b05-8c82-a710dc440413"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 3, 32, 32]) tensor(65)\n",
            "rabbit\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for img, lbl in train_loader:\n",
        "    print(img.shape, lbl[0])\n",
        "    print(train_data.classes[lbl[0]])\n",
        "    plt.imshow(img[0].permute(1, 2, 0))\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "JOuEWVcISG9r",
        "outputId": "9f2714b4-a9d8-45f7-ab59-290156379672"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 3, 32, 32]) tensor(49)\n",
            "mountain\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "for img, lbl in valid_loader:\n",
        "    print(img.shape, lbl[0])\n",
        "    print(valid_data.classes[lbl[0]])\n",
        "    plt.imshow(img[0].permute(1, 2, 0))\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# utils"
      ],
      "metadata": {
        "id": "Rl9-ODRlryKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "\n",
        "def train_util(model,\n",
        "          criterion,\n",
        "          optimizer,\n",
        "          train_dataloader,\n",
        "          test_dataloader,\n",
        "          print_every,\n",
        "          num_epoch,\n",
        "          # adam_optimizer_lr=0.001\n",
        "          ):\n",
        "  \n",
        "    steps = 0\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "    model.to(device)\n",
        "    for epoch in tqdm(range(num_epoch)):\n",
        "        running_loss = 0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "        start_time = time()\n",
        "        iter_time = time()\n",
        "        \n",
        "        model.train()\n",
        "        for i, (images, labels) in enumerate(train_dataloader):\n",
        "            steps += 1\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(images)\n",
        "            loss = criterion(output, labels)\n",
        "\n",
        "            correct_train += (torch.max(output, dim=1)[1] == labels).sum()\n",
        "            total_train += labels.size(0)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Logging\n",
        "            if steps % print_every == 0:\n",
        "                print(f'Epoch [{epoch + 1}]/[{num_epoch}]. Batch [{i + 1}]/[{len(train_dataloader)}].', end=' ')\n",
        "                print(f'Train loss {running_loss / steps:.3f}.', end=' ')\n",
        "                print(f'Train acc {correct_train / total_train * 100:.3f}.', end=' ')\n",
        "                with torch.no_grad():\n",
        "                    model.eval()\n",
        "                    correct_val, total_val = 0, 0\n",
        "                    val_loss = 0\n",
        "                    for images, labels in test_dataloader:\n",
        "                        images = images.to(device)\n",
        "                        labels = labels.to(device)\n",
        "                        output = model(images)\n",
        "                        loss = criterion(output, labels)\n",
        "                        val_loss += loss.item()\n",
        "\n",
        "                        correct_val += (torch.max(output, dim=1)[1] == labels).sum()\n",
        "                        total_val += labels.size(0)\n",
        "\n",
        "                print(f'Val loss {val_loss / len(test_dataloader):.3f}. Val acc {correct_val / total_val * 100:.3f}.', end=' ')\n",
        "                print(f'({time() - iter_time:.3f} seconds)')\n",
        "                iter_time = time()\n",
        "\n",
        "                train_losses.append(running_loss / total_train)\n",
        "                val_losses.append(val_loss / total_val)\n",
        "\n",
        "        torch.save(model, f'checkpoint_{correct_val / total_val * 100:.2f}')\n",
        "        \n",
        "    return model, train_losses, val_losses"
      ],
      "metadata": {
        "id": "rQwb8pFspPgJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# my CNN model"
      ],
      "metadata": {
        "id": "L_bkhcffMFCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCifar100Net(nn.Module): \n",
        "\n",
        "    def __init__(self, num_classes:int=1000):\n",
        "      \n",
        "        super(MyCifar100Net, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            \n",
        "          nn.BatchNorm2d(3),\n",
        "          nn.Conv2d(in_channels=3, out_channels=30, kernel_size=3, padding=1),\n",
        "          nn.ReLU(inplace=True),   \n",
        "          nn.Conv2d(in_channels=30, out_channels=60, kernel_size=3, padding=0),\n",
        "          nn.ReLU(inplace=True),       \n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "          nn.BatchNorm2d(60),\n",
        "          nn.Conv2d(in_channels=60, out_channels=120, kernel_size=3, padding=1),\n",
        "          nn.ReLU(inplace=True),  \n",
        "          nn.Conv2d(in_channels=120, out_channels=120, kernel_size=3, padding=0), \n",
        "          nn.ReLU(inplace=True),      \n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "          nn.BatchNorm2d(120),\n",
        "          nn.Conv2d(in_channels=120, out_channels=240, kernel_size=3, padding=1),\n",
        "          nn.LeakyReLU(inplace=True, negative_slope=0.1),   \n",
        "          nn.Conv2d(in_channels=240, out_channels=480, kernel_size=3, padding=0), \n",
        "          nn.LeakyReLU(inplace=True, negative_slope=0.1),       \n",
        "          nn.MaxPool2d(kernel_size=2),        \n",
        "\n",
        "          nn.BatchNorm2d(480),          \n",
        "          nn.Flatten(),\n",
        "          nn.Dropout(0.25), \n",
        "         \n",
        "          nn.Linear(480*2*2, 1000),  \n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.Dropout(0.5),        \n",
        "\n",
        "          nn.Linear(1000, num_classes))        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.layers(x)  \n",
        "       \n",
        "net = MyCifar100Net(num_classes=num_classes).to(device)\n",
        "print(net)\n",
        "summary(net.to(device), input_size=(3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmnAjK17xAGX",
        "outputId": "e9532063-d3e6-42ea-90e6-92d264a59cf9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyCifar100Net(\n",
            "  (layers): Sequential(\n",
            "    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (1): Conv2d(3, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(30, 60, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): Conv2d(60, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (10): ReLU(inplace=True)\n",
            "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (12): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (13): Conv2d(120, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (14): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "    (15): Conv2d(240, 480, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (16): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (18): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): Flatten(start_dim=1, end_dim=-1)\n",
            "    (20): Dropout(p=0.25, inplace=False)\n",
            "    (21): Linear(in_features=1920, out_features=1000, bias=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): Dropout(p=0.5, inplace=False)\n",
            "    (24): Linear(in_features=1000, out_features=100, bias=True)\n",
            "  )\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "       BatchNorm2d-1            [-1, 3, 32, 32]               6\n",
            "            Conv2d-2           [-1, 30, 32, 32]             840\n",
            "              ReLU-3           [-1, 30, 32, 32]               0\n",
            "            Conv2d-4           [-1, 60, 30, 30]          16,260\n",
            "              ReLU-5           [-1, 60, 30, 30]               0\n",
            "         MaxPool2d-6           [-1, 60, 15, 15]               0\n",
            "       BatchNorm2d-7           [-1, 60, 15, 15]             120\n",
            "            Conv2d-8          [-1, 120, 15, 15]          64,920\n",
            "              ReLU-9          [-1, 120, 15, 15]               0\n",
            "           Conv2d-10          [-1, 120, 13, 13]         129,720\n",
            "             ReLU-11          [-1, 120, 13, 13]               0\n",
            "        MaxPool2d-12            [-1, 120, 6, 6]               0\n",
            "      BatchNorm2d-13            [-1, 120, 6, 6]             240\n",
            "           Conv2d-14            [-1, 240, 6, 6]         259,440\n",
            "        LeakyReLU-15            [-1, 240, 6, 6]               0\n",
            "           Conv2d-16            [-1, 480, 4, 4]       1,037,280\n",
            "        LeakyReLU-17            [-1, 480, 4, 4]               0\n",
            "        MaxPool2d-18            [-1, 480, 2, 2]               0\n",
            "      BatchNorm2d-19            [-1, 480, 2, 2]             960\n",
            "          Flatten-20                 [-1, 1920]               0\n",
            "          Dropout-21                 [-1, 1920]               0\n",
            "           Linear-22                 [-1, 1000]       1,921,000\n",
            "             ReLU-23                 [-1, 1000]               0\n",
            "          Dropout-24                 [-1, 1000]               0\n",
            "           Linear-25                  [-1, 100]         100,100\n",
            "================================================================\n",
            "Total params: 3,530,886\n",
            "Trainable params: 3,530,886\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.64\n",
            "Params size (MB): 13.47\n",
            "Estimated Total Size (MB): 16.12\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_loader, valid_loader = get_dataloaders(train_data, valid_data, train_transforms, valid_transforms)\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr_cnn[0])\n",
        "\n",
        "net, train_losses, val_losses = train_util(\n",
        "    model=net,\n",
        "    criterion=criterion,\n",
        "    optimizer = optimizer,\n",
        "    train_dataloader=train_loader,\n",
        "    test_dataloader=valid_loader,\n",
        "    print_every=print_every,\n",
        "    num_epoch=num_epochs\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHK8VV3chKY5",
        "outputId": "dd6816be-6ccd-434f-899d-d2b0be3be9e5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1]/[10]. Batch [300]/[391]. Train loss 3.717. Train acc 14.523. Val loss 3.046. Val acc 25.140. (15.860 seconds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 1/10 [00:18<02:49, 18.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2]/[10]. Batch [209]/[391]. Train loss 0.961. Train acc 30.943. Val loss 2.514. Val acc 35.490. (9.430 seconds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:34<02:13, 16.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3]/[10]. Batch [118]/[391]. Train loss 0.288. Train acc 41.757. Val loss 2.221. Val acc 42.070. (6.367 seconds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:49<01:51, 15.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4]/[10]. Batch [27]/[391]. Train loss 0.041. Train acc 50.463. Val loss 2.049. Val acc 45.640. (3.597 seconds)\n",
            "Epoch [4]/[10]. Batch [327]/[391]. Train loss 0.359. Train acc 54.673. Val loss 2.007. Val acc 47.430. (11.974 seconds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [01:06<01:39, 16.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5]/[10]. Batch [236]/[391]. Train loss 0.216. Train acc 53.959. Val loss 1.895. Val acc 49.750. (10.170 seconds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [01:21<01:20, 16.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6]/[10]. Batch [145]/[391]. Train loss 0.097. Train acc 59.871. Val loss 1.875. Val acc 51.050. (7.258 seconds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [01:37<01:04, 16.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7]/[10]. Batch [54]/[391]. Train loss 0.025. Train acc 66.319. Val loss 1.839. Val acc 52.460. (5.174 seconds)\n",
            "Epoch [7]/[10]. Batch [354]/[391]. Train loss 0.103. Train acc 76.311. Val loss 2.116. Val acc 51.330. (12.279 seconds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [01:56<00:50, 16.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8]/[10]. Batch [263]/[391]. Train loss 0.100. Train acc 66.109. Val loss 1.890. Val acc 52.520. (10.966 seconds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [02:11<00:32, 16.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9]/[10]. Batch [172]/[391]. Train loss 0.050. Train acc 70.935. Val loss 1.928. Val acc 53.470. (8.093 seconds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [02:26<00:15, 15.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10]/[10]. Batch [81]/[391]. Train loss 0.019. Train acc 74.566. Val loss 1.914. Val acc 54.020. (5.221 seconds)\n",
            "Epoch [10]/[10]. Batch [381]/[391]. Train loss 0.038. Train acc 87.793. Val loss 2.657. Val acc 52.080. (11.955 seconds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [02:43<00:00, 16.39s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# дообучение ImageNet Resnet-50"
      ],
      "metadata": {
        "id": "CvRwH0EM8bqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "\n",
        "def resnet_to_cifar100():\n",
        "  _resnet = models.resnet50(pretrained=True)\n",
        "\n",
        "  # Заморозим претренерованные слои, чтобы они не обучались.\n",
        "  for param in list(_resnet.parameters())[:]:\n",
        "      param.requires_grad = False\n",
        "\n",
        "  # replace the last linear layer\n",
        "  in_features = _resnet.fc.in_features\n",
        "  _resnet.fc = nn.Linear(in_features=in_features, out_features=num_classes)\n",
        "  _resnet = _resnet.to(device)\n",
        "\n",
        "  # current trainable params\n",
        "  _params_to_update = []\n",
        "  for name, param in _resnet.named_parameters():\n",
        "      if param.requires_grad == True:\n",
        "          _params_to_update.append(param)\n",
        "\n",
        "  return _resnet, _params_to_update"
      ],
      "metadata": {
        "id": "mt-gt_acISxh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet, params_to_update = resnet_to_cifar100()"
      ],
      "metadata": {
        "id": "PspovV3KcPnl"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(resnet.to(device), input_size=(3, 224, 224)), len(params_to_update)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hkon6rvxe8Wf",
        "outputId": "fcbefa0f-d11b-4124-a4fa-4f7267e4a610"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
            "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
            "             ReLU-15          [-1, 256, 56, 56]               0\n",
            "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
            "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
            "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
            "             ReLU-19           [-1, 64, 56, 56]               0\n",
            "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
            "             ReLU-22           [-1, 64, 56, 56]               0\n",
            "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
            "             ReLU-25          [-1, 256, 56, 56]               0\n",
            "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
            "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
            "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
            "             ReLU-29           [-1, 64, 56, 56]               0\n",
            "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
            "             ReLU-32           [-1, 64, 56, 56]               0\n",
            "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
            "             ReLU-35          [-1, 256, 56, 56]               0\n",
            "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
            "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
            "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
            "             ReLU-39          [-1, 128, 56, 56]               0\n",
            "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
            "             ReLU-42          [-1, 128, 28, 28]               0\n",
            "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
            "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
            "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-47          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
            "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
            "             ReLU-51          [-1, 128, 28, 28]               0\n",
            "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
            "             ReLU-54          [-1, 128, 28, 28]               0\n",
            "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-57          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
            "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
            "             ReLU-61          [-1, 128, 28, 28]               0\n",
            "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
            "             ReLU-64          [-1, 128, 28, 28]               0\n",
            "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-67          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
            "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
            "             ReLU-71          [-1, 128, 28, 28]               0\n",
            "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
            "             ReLU-74          [-1, 128, 28, 28]               0\n",
            "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-77          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
            "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
            "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
            "             ReLU-81          [-1, 256, 28, 28]               0\n",
            "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
            "             ReLU-84          [-1, 256, 14, 14]               0\n",
            "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
            "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
            "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
            "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
            "             ReLU-89         [-1, 1024, 14, 14]               0\n",
            "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
            "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
            "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
            "             ReLU-93          [-1, 256, 14, 14]               0\n",
            "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
            "             ReLU-96          [-1, 256, 14, 14]               0\n",
            "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
            "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
            "             ReLU-99         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
            "            ReLU-103          [-1, 256, 14, 14]               0\n",
            "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
            "            ReLU-106          [-1, 256, 14, 14]               0\n",
            "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-109         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
            "            ReLU-113          [-1, 256, 14, 14]               0\n",
            "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
            "            ReLU-116          [-1, 256, 14, 14]               0\n",
            "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-119         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
            "            ReLU-123          [-1, 256, 14, 14]               0\n",
            "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
            "            ReLU-126          [-1, 256, 14, 14]               0\n",
            "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-129         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
            "            ReLU-133          [-1, 256, 14, 14]               0\n",
            "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
            "            ReLU-136          [-1, 256, 14, 14]               0\n",
            "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-139         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
            "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
            "            ReLU-143          [-1, 512, 14, 14]               0\n",
            "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-146            [-1, 512, 7, 7]               0\n",
            "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
            "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
            "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-151           [-1, 2048, 7, 7]               0\n",
            "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
            "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-155            [-1, 512, 7, 7]               0\n",
            "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-158            [-1, 512, 7, 7]               0\n",
            "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-161           [-1, 2048, 7, 7]               0\n",
            "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
            "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-165            [-1, 512, 7, 7]               0\n",
            "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-168            [-1, 512, 7, 7]               0\n",
            "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-171           [-1, 2048, 7, 7]               0\n",
            "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
            "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
            "          Linear-174                  [-1, 100]         204,900\n",
            "================================================================\n",
            "Total params: 23,712,932\n",
            "Trainable params: 204,900\n",
            "Non-trainable params: 23,508,032\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 286.55\n",
            "Params size (MB): 90.46\n",
            "Estimated Total Size (MB): 377.58\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, valid_loader = get_dataloaders(train_data, valid_data, train_transforms, valid_transforms)"
      ],
      "metadata": {
        "id": "hB-WTgRWfxyI"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params_to_update, lr=lr_cnn[0])\n",
        "\n",
        "resnet, train_losses, val_losses = train_util(\n",
        "    model=resnet,\n",
        "    criterion=criterion, \n",
        "    optimizer=optimizer,   \n",
        "    train_dataloader=train_loader,\n",
        "    test_dataloader=valid_loader,\n",
        "    print_every=print_every,\n",
        "    num_epoch=num_epochs\n",
        ")"
      ],
      "metadata": {
        "id": "wbL-xC-AhXH8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "529b3ade-c598-472b-bcdd-02d3a198c5d2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1]/[10]. Batch [300]/[391]. Train loss 3.566. Train acc 20.383. Val loss 3.220. Val acc 26.170. (13.922 seconds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 1/10 [00:17<02:35, 17.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2]/[10]. Batch [209]/[391]. Train loss 1.030. Train acc 29.912. Val loss 3.122. Val acc 27.920. (12.332 seconds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:36<02:25, 18.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3]/[10]. Batch [118]/[391]. Train loss 0.360. Train acc 33.283. Val loss 3.093. Val acc 28.610. (7.448 seconds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:53<02:04, 17.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4]/[10]. Batch [27]/[391]. Train loss 0.058. Train acc 37.471. Val loss 3.065. Val acc 29.730. (4.206 seconds)\n",
            "Epoch [4]/[10]. Batch [327]/[391]. Train loss 0.552. Train acc 38.069. Val loss 3.059. Val acc 29.460. (13.373 seconds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [01:13<01:51, 18.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5]/[10]. Batch [236]/[391]. Train loss 0.333. Train acc 37.169. Val loss 3.059. Val acc 29.570. (11.747 seconds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [01:30<01:30, 18.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6]/[10]. Batch [145]/[391]. Train loss 0.169. Train acc 38.960. Val loss 3.027. Val acc 30.540. (8.446 seconds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [01:48<01:11, 17.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7]/[10]. Batch [54]/[391]. Train loss 0.052. Train acc 41.638. Val loss 3.039. Val acc 30.420. (5.260 seconds)\n",
            "Epoch [7]/[10]. Batch [354]/[391]. Train loss 0.294. Train acc 43.529. Val loss 3.051. Val acc 30.700. (13.428 seconds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [02:08<00:56, 18.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8]/[10]. Batch [263]/[391]. Train loss 0.207. Train acc 40.378. Val loss 3.019. Val acc 31.010. (12.612 seconds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [02:25<00:36, 18.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9]/[10]. Batch [172]/[391]. Train loss 0.120. Train acc 41.642. Val loss 3.083. Val acc 30.400. (9.345 seconds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [02:43<00:18, 18.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10]/[10]. Batch [81]/[391]. Train loss 0.050. Train acc 43.615. Val loss 3.043. Val acc 30.700. (6.131 seconds)\n",
            "Epoch [10]/[10]. Batch [381]/[391]. Train loss 0.200. Train acc 47.363. Val loss 3.051. Val acc 30.660. (13.464 seconds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [03:03<00:00, 18.37s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDs0HOHIQjjH"
      },
      "source": [
        "# дообучение ImageNet Resnet-50 с аугментацией данных"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_aug, params_to_update = resnet_to_cifar100()"
      ],
      "metadata": {
        "id": "D7kjHgNKQyjz"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summary(resnet_aug.to(device), input_size=(3, 224, 224)), len(params_to_update)"
      ],
      "metadata": {
        "id": "_1qCuaZAfROq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(params_to_update)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnPSqJ2Kn3ie",
        "outputId": "4fcf966d-b777-49d5-f276-e2fe37ff7cb7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data transforms (normalization & data augmentation)\n",
        "transforms_augmentation = transforms.Compose([\n",
        "                         transforms.RandomCrop(32, padding=4, padding_mode='reflect'), \n",
        "                         transforms.RandomHorizontalFlip(),                          \n",
        "                         transforms.ToTensor(), \n",
        "                         transforms.Normalize(*stats,inplace=True)])\n",
        "\n",
        "train_loader, valid_loader = get_dataloaders(train_data, valid_data, transforms_augmentation, valid_transforms)\n",
        "\n",
        "optimizer = torch.optim.Adam(params_to_update, lr=lr_cnn[0])\n",
        "\n",
        "resnet_aug, train_losses, val_losses = train_util(\n",
        "    model=resnet_aug,\n",
        "    criterion=criterion, \n",
        "    optimizer=optimizer,   \n",
        "    train_dataloader=train_loader,\n",
        "    test_dataloader=valid_loader,\n",
        "    print_every=print_every,\n",
        "    num_epoch=num_epochs\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6hewswFSLHa",
        "outputId": "e43b36a8-85d4-496a-e4d2-9b5fe870e37b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1]/[10]. Batch [300]/[391]. Train loss 3.637. Train acc 18.914. Val loss 3.255. Val acc 25.790. (22.263 seconds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 1/10 [00:27<04:11, 27.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2]/[10]. Batch [209]/[391]. Train loss 1.109. Train acc 25.796. Val loss 3.097. Val acc 27.970. (16.544 seconds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:55<03:43, 27.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3]/[10]. Batch [118]/[391]. Train loss 0.401. Train acc 28.277. Val loss 3.077. Val acc 28.550. (10.765 seconds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [01:25<03:22, 28.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4]/[10]. Batch [27]/[391]. Train loss 0.068. Train acc 29.948. Val loss 3.034. Val acc 30.010. (7.872 seconds)\n",
            "Epoch [4]/[10]. Batch [327]/[391]. Train loss 0.654. Train acc 30.036. Val loss 3.029. Val acc 30.100. (22.765 seconds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [02:00<03:07, 31.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5]/[10]. Batch [236]/[391]. Train loss 0.386. Train acc 29.946. Val loss 2.967. Val acc 30.730. (18.309 seconds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [02:29<02:31, 30.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6]/[10]. Batch [145]/[391]. Train loss 0.200. Train acc 30.797. Val loss 2.950. Val acc 30.560. (12.586 seconds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [02:57<01:58, 29.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7]/[10]. Batch [54]/[391]. Train loss 0.066. Train acc 30.223. Val loss 2.963. Val acc 30.750. (6.758 seconds)\n",
            "Epoch [7]/[10]. Batch [354]/[391]. Train loss 0.377. Train acc 31.864. Val loss 2.924. Val acc 32.080. (22.356 seconds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [03:29<01:30, 30.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8]/[10]. Batch [263]/[391]. Train loss 0.250. Train acc 31.779. Val loss 2.928. Val acc 31.760. (19.985 seconds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [03:57<00:58, 29.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9]/[10]. Batch [172]/[391]. Train loss 0.147. Train acc 32.140. Val loss 2.931. Val acc 31.970. (14.284 seconds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [04:24<00:28, 28.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10]/[10]. Batch [81]/[391]. Train loss 0.063. Train acc 32.996. Val loss 2.906. Val acc 31.670. (8.645 seconds)\n",
            "Epoch [10]/[10]. Batch [381]/[391]. Train loss 0.273. Train acc 33.374. Val loss 2.905. Val acc 31.670. (22.117 seconds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [04:56<00:00, 29.63s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "качество на самописной модели выше, но с большой разницей на трейне и тесте.\n",
        "\n",
        "улучшение в направлнии:\n",
        "1. больше итераций (для всех моделей), \n",
        "2. более разнообразная аугментация (наверно)\n",
        "3. постепенно замещать предпоследние слои в предобученной модели"
      ],
      "metadata": {
        "id": "4JJXcc4soEQ2"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "HW_4.ipynb",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}